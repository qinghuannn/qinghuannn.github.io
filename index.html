<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Ling-An Zeng</title>
    
    <script>
      const WEBSITE_URL = "https://lingan.art";
    </script>

    <meta name="author" content="Ling-An Zeng">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- SEO Improvements - Meta Description -->
    <meta name="description" content="Ling-An Zeng is a PhD student at Sun Yat-sen University specializing in video understanding, human-related technologies, and AIGC. View publications and research.">
    <!-- SEO Improvements - Keywords -->
    <meta name="keywords" content="Ling-An Zeng, Computer Vision, Video Understanding, Human-Object Interaction, AIGC, Action Quality Assessment, Sun Yat-sen University, AI Research">
    <!-- Canonical URL -->
    <meta rel="canonical" href="https://lingan.art">
    
    <!-- Social Media Meta Tags - Open Graph -->
    <meta property="og:title" content="Ling-An Zeng">
    <meta property="og:description" content="PhD student at Sun Yat-sen University researching video understanding and human-related technologies.">
    <meta property="og:image" content="https://lingan.art/images/myself.jpg">
    <meta property="og:url" content="https://lingan.art">
    <meta property="og:type" content="website">
    
    <!-- Twitter Card Tags -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Ling-An Zeng">
    <meta name="twitter:description" content="PhD student at Sun Yat-sen University researching video understanding and human-related technologies.">
    <meta name="twitter:image" content="https://lingan.art/images/myself.jpg">
    
    <link rel="shortcut icon" href="images/ico.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel='stylesheet' href='https://chinese-fonts-cdn.deno.dev/packages/xuandongkaishu/dist/XuandongKaishu/result.css' />

    <!-- Standard favicon -->
    <link rel="icon" type="image/png" sizes="32x32" href="images/ico.png">
    <link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
    <!-- Google specifically looks for this -->
    <link rel="icon" type="image/png" sizes="48x48" href="images/favicon-48x48.png">
    <!-- Apple Touch Icon -->
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
    <!-- Web App Manifest -->
    <link rel="manifest" href="site.webmanifest">

    <!-- Add global font size adjustment -->
    <style>
      html, body {
        font-size: 20px; /* Increase base font size */
      }
      /* Ensure all text elements inherit the proper font size */
      p, div, span, li, td {
        font-size: 1.05em; /* This ensures proper inheritance from the body */
      }
      
      /* Adjust specific elements that might need proportional scaling */
      h2 {
        font-size: 2em;
      }
      
      .papertitle {
        font-size: 1.1em;
        font-weight: 600;
      }
      
      .header_navi {
        font-size: 1em;
      }
      /* Ensure links are properly sized */
      a {
        font-size: inherit;
      }
      
      /* Make sure biography text is clearly legible */
      #bio + p, #bio ~ p {
        line-height: 1.5; /* Improved line spacing for readability */
      }
      
      /* Adjust the scrollable news section */
      .scrollable p {
        font-size: 0.95em;
        line-height: 1.4;
      }
    </style>
    
  </head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ERLNRPTKNV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-ERLNRPTKNV');
  </script>
  <body>

    

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom: 30px"><tbody>
      <tr style="padding:0px">
        <td class="fixed-header" style="font-size: larger">
          <a href="javascript:void(0)" onclick="window.location.href=WEBSITE_URL" style="padding-right: 150px; font-size: large;" class="header_name"> Ling-An Zeng</a>
          <!-- <a href="javascript:void(0)" onclick="window.location.href=WEBSITE_URL" style="padding-right: 150px; font-size: large;" class="header_name"><img src="images/ico.png" style="width: 20px"> Ling-An Zeng</a> -->
          

          <a href="#bio" style="padding-right: 30px;padding-left: 30px" class="header_navi">Bio</a>
          <a href="#publication" style="padding-right: 30px;padding-left: 30px" class="header_navi">Publications</a>
          <a href="#service" style="padding-right: 30px;padding-left: 30px" class="header_navi">Services</a>
          <a href="#award" style="padding-right: 30px;padding-left: 30px" class="header_navi">Awards</a>
          <a href="cv/cv_en.pdf" target="_blank" style="padding-right: 30px;padding-left: 30px" class="header_navi">CV</a>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td class="personal_image" style="padding:2.5%; width:40%;max-width:40%">
                <img style="width:70%;max-width:100%;object-fit: cover;border-radius: 200px" alt="profile photo" src="images/myself.jpg" class="personal_img">
              </td>

              <td style="width:63%;vertical-align:middle">
                <p class="name" style="text-align: left;">
                  Ling-An Zeng <span style="padding-left: 5px" class="chinese_name">曾令安</span>
                </p>
                <p>
                  Ph.D. student
                </p>
                <p>
                  Sun Yat-sen University
                </p>
                <p>
                  Email: linganzeng@gmail.com
                </p>
                <p style="text-align:left">
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=Uyx8l8sAAAAJ" target="_blank">
                    [<span style="color: #4285F4; margin-right: -2px">G</span>
                    <span style="color: #EA4335; margin-right: -2px">o</span>
                    <span style="color: #FBBC05; margin-right: -2px">o</span>
                    <span style="color: #4285F4; margin-right: -2px">g</span>
                    <span style="color: #34A853; margin-right: -2px">l</span>
                    <span style="color: #EA4335; margin-right: -2px">e</span> &nbsp Scholar]</a> &nbsp;&nbsp;
                  <!-- <a href="https://github.com/DravenALG" target="_blank"><strong style="color: black">[GitHub]</strong></a> -->
                </p>
              </td>

            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td style="width:100%; vertical-align:middle">
                <h2 id="bio" style="scroll-margin-top: 45px;">
                  Biography
                </h2>
                <p>
                  I'm currently a 4th-year Ph.D. student at Sun Yat-sen University, advised by Prof. <a href="https://www.isee-ai.cn/~zhwshi/" target="_blank">Wei-Shi Zheng</a>. My research interests include <strong>video understanding</strong> and <strong>human-related technologies</strong>. I also have a strong interest in <strong>AIGC</strong>, such as image and video generation. 
                </p>
                <p>
                    Previously, I obtained my M.S. degree from Sun Yat-sen University in 2021, advised by Prof. <a href="https://www.isee-ai.cn/~zhwshi/" target="_blank">Wei-Shi Zheng</a>. Before that, I obtained my B.E. degree from the University of Electronic Science and Technology of China in 2019.
                </p>
                <p>
                    <span style="color: red; font-weight: bold;">I'm seeking full-time positions/postdoc/intern starting in 2025. :)</span> 
                    Here is my <a href="cv/cv_en.pdf" target="_blank">CV</a>.
                </p>
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
          <tr>
            <td style="width:100%; vertical-align:middle">
              <h2>
                News
              </h2>
              <div class="scrollable" style="max-height:150px; overflow-y:scroll; padding-right:10px; margin-top: 10px; margin-bottom: 10px">
                <p>
                  <span style="font-size: smaller">➤</span> [2025-02] One paper accepted in CVPR 2025.
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2024-12] One paper accepted in AAAI 2025.
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2024-07] One paper accepted in ECCV 2024.
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2024-05] One paper accepted in IEEE Transactions on Circuits and Systems for Video Technology (TCSVT).
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2024-04] One paper accepted in IEEE Transactions on Multimedia (TMM).
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2024-02] One paper accepted in IEEE Transactions on Image Processing (TIP).                  .
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2022-03] One paper accepted in CVPR 2022.
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2020-07] One paper accepted in ACM Multimedia 2020.
                </p>
              </div>
            </td>
          </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 0px; margin-bottom: 10px;"><tbody>
            <tr>
              <td style="width:80%; vertical-align:middle">
                <h2 id="publication" style="scroll-margin-top: 45px;">Publications</h2>
                <p>
                  Below are my publications.
                  († means equal contribution, * refers to corresponding author.)
                  <button id="showFullListBtn" class="toggle-button" onclick="showFullList()">[Show Full List]</button>
                  <button id="showSelectedListBtn" class="toggle-button" onclick="showSelectedList()" style="display:none;">[Show Selected List]</button>
                </p>
              </td>
            </tr>
           </tbody></table>

          <table id="selectedPublications" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/CHAINHOI.jpg" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://arxiv.org/abs/todo" target="_blank">
                  <span class="papertitle">ChainHOI: Joint-based Kinematic Chain Modeling for Human-Object Interaction Generation</span>
                </a>
                <br>
                <strong>Ling-An Zeng†</strong>, Guohong Huang†, Yi-Lin Wei, Shengbo Gu, Yu-Ming Tang, Jingke Meng*, Wei-Shi Zheng*.
                <br>
                <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025.
                <!-- <br>
                <a href="https://arxiv.org/abs/2308.15989" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/DiffuVolume" target="_blank">code</a>
                 -->
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>ChainHOI</strong>: Text-driven human-object interaction generation with explicit joint and kinetic chain modeling for realistic results.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/light-t2m.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://arxiv.org/abs/2412.11193" target="_blank">
                  <span class="papertitle">Light-T2M: A Lightweight and Fast Model for Text-to-motion Generation</span>
                </a>
                <br>
                <strong>Ling-An Zeng</strong>, Guohong Huang, Gaojie Wu, Wei-Shi Zheng*.
                <br>
                <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2025.
                <br>
                <a href="https://qinghuannn.github.io/light-t2m/" target="_blank">project</a>
                /
                <a href="https://arxiv.org/abs/2412.11193" target="_blank">paper</a>
                /
                <a href="https://github.com/qinghuannn/light-t2m" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>Light-T2M</strong>: A Lightweight and Fast Model for Text-to-motion Generation.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/egoexo_fitness.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://arxiv.org/abs/2406.08877" target="_blank">
                  <span class="papertitle">EgoExo-Fitness: Towards Egocentric and Exocentric Full-Body Action Understanding</span>
                </a>
                <br>
                Yuan-Ming Li†, Wei-Jin Huang†, An-Lan Wang†, <strong>Ling-An Zeng</strong>, Jing-Ke Meng*, Wei-Shi Zheng*
                <br>
                <em>European Conference on Computer Vision (ECCV)</em>, 2024.
                <br>
                <a href="https://arxiv.org/abs/2406.08877" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/EgoExo-Fitness/tree/main" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>EgoExo-Fitness</strong>: Synchronized egocentric and exocentric views for action understanding with action quality annotations.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/tgaw.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/10529983/" target="_blank">
                  <span class="papertitle">Adaptive Weight Generator for Multi-Task Image Recognition by Task Grouping Prompt</span>
                </a>
                <br>
                Gaojie Wu, <strong>Ling-an Zeng</strong>, Jing-Ke Meng*, Wei-Shi Zheng
                <br>
                <em>IEEE Transactions on Multimedia (TMM)</em>, 2024.
                <br>
                <a href="https://ieeexplore.ieee.org/abstract/document/10529983/" target="_blank">paper</a>
                <!-- / -->
                <!-- <a href="https://github.com/iSEE-Laboratory/EconomicGrasp" target="_blank">code</a> -->
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>TGAW</strong>: Efficient multi-task image recognition via prompt-based automatic task grouping.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/continual_aqa.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/10518028" target="_blank">
                  <span class="papertitle">Continual Action Assessment via Task-Consistent Score-Discriminative Feature Distribution Modeling</span>
                </a>
                <br>
                Yuan-Ming Li, <strong>Ling-An Zeng</strong>, Jing-Ke Meng*, Wei-Shi Zheng*.
                <br>
                <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em>, 2024.
                <br>
                <a href="https://ieeexplore.ieee.org/document/10518028" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/Continual-AQA" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>Continual-AQA</strong>: Enabling sequential learning in Action Quality Assessment without forgetting, using innovative rehearsal and graph-based techniques for superior performance.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/pamfn.jpg" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://arxiv.org/pdf/2402.09444" target="_blank">
                  <span class="papertitle">Multimodal action quality assessment</span>
                </a>
                <br>
                <strong>Ling-An Zeng</strong>, Wei-Shi Zheng*.
                <br>
                <em>IEEE Transactions on Image Processing (TIP)</em>, 2024.
                <br>
                <a href="https://arxiv.org/pdf/2402.09444" target="_blank">paper</a>
                /
                <a href="https://github.com/qinghuannn/PAMFN" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>PAMFN</strong>: State-of-the-art action quality assessment via adaptive fusion of RGB, optical flow, and audio.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/gdlt.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xu_Likert_Scoring_With_Grade_Decoupling_for_Long-Term_Action_Assessment_CVPR_2022_paper.html" target="_blank">
                  <span class="papertitle">Likert Scoring With Grade Decoupling for Long-Term Action Assessment</span>
                </a>
                <br>
                Angchi Xu, <strong>Ling-An Zeng</strong>, Wei-Shi Zheng*.
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022.
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xu_Likert_Scoring_With_Grade_Decoupling_for_Long-Term_Action_Assessment_CVPR_2022_paper.html" target="_blank">paper</a>
                /
                <a href="https://github.com/xuangch/CVPR22_GDLT" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>GDLT</strong>: Quantifying performance grades in long-term action quality assessment for state-of-the-art results.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/action-net.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413560" target="_blank">
                  <span class="papertitle">Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos</span>
                </a>
                <br>
                <strong>Ling-An Zeng</strong>, Fa-Ting Hong, Wei-Shi Zheng*, Qi-Zhi Yu, Wei Zeng, Yao-Wei Wang, Jian-Huang Lai.
                <br>
                <em>ACM international conference on multimedia (ACM MM)</em>, 2020.
                <br>
                <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413560" target="_blank">paper</a>
                /
                <a href="https://github.com/qinghuannn/ACTION-NET" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>ACTION-NET</strong>: State-of-the-art action quality assessment in long sports videos using hybrid dynamic-static modeling and attention, introducing the Rhythmic Gymnastics dataset.
                </p>
              </td>
            </tr>


          </tbody></table>
          

          <table id="fullPublications" style="display:none; width:100%;border:0px;border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto; padding-left:20px;"><tbody">
            <tr>
              <td style="padding-left: 20px; padding-bottom: 25px ; width:100%; vertical-align:middle">
                <span class="papertitle">ChainHOI: Joint-based Kinematic Chain Modeling for Human-Object Interaction Generation</span>
                <br>
                <strong>Ling-An Zeng†</strong>, Guohong Huang†, Yi-Lin Wei, Shengbo Gu, Yu-Ming Tang, Jingke Meng*, Wei-Shi Zheng*
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025.
                <br>
              </td>
            </tr>
            
            <tr>
              <td style="padding-left: 20px; padding-bottom: 25px ; width:100%; vertical-align:middle">
                <span class="papertitle">Light-T2M: A Lightweight and Fast Model for Text-to-motion Generation</span>
                <br>
                <strong>Ling-An Zeng</strong>, Guohong Huang, Gaojie Wu, Wei-Shi Zheng*
                <br>
                <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2025.
                <br>
              </td>
            </tr>

            <tr>
              <td style="padding-left: 20px; padding-bottom: 25px ; width:100%; vertical-align:middle">
                <span class="papertitle">TechCoach: Towards Technical Keypoint-Aware Descriptive Action Coaching</span>
                <br>
                Yuan-Ming Li‡†, An-Lan Wang†, Kun-Yu Lin, Yu-Ming Tang, <strong>Ling-An Zeng</strong>, Jian-Fang Hu, Wei-Shi Zheng*
                <br>
                <em>arXiv</em>, 2024.
                <br>
              </td>
            </tr>

            <tr>
              <td style="padding-left: 20px; padding-bottom: 25px ; width:100%; vertical-align:middle">
                <span class="papertitle">Privacy-Preserving Action Recognition: A Survey</span>
                <br>
                Xiao Li, Yu-Kun Qiu, Yi-Xing Peng, <strong>Ling-An Zeng</strong>, Wei-Shi Zheng
                <br>
                <em>Chinese Conference on Pattern Recognition and Computer Vision (PRCV)</em>, 2024.
                <br>
              </td>
            </tr>
            
            <tr>
              <td style="padding-left: 20px; padding-bottom: 25px ; width:100%; vertical-align:middle">
                <span class="papertitle">EgoExo-Fitness: Towards Egocentric and Exocentric Full-Body Action Understanding</span>
                <br>
                Yuan-Ming Li†, Wei-Jin Huang†, An-Lan Wang†, <strong>Ling-An Zeng</strong>, Jing-Ke Meng*, Wei-Shi Zheng*
                <br>
                <em>European Conference on Computer Vision (ECCV)</em>, 2024.
                <br>
              </td>
            </tr>

            <tr>
              <td style="padding-left: 20px; padding-bottom: 25px ; width:100%; vertical-align:middle">
                <span class="papertitle">Adaptive Weight Generator for Multi-Task Image Recognition by Task Grouping Prompt</span>
                <br>
                Gaojie Wu, <strong>Ling-an Zeng</strong>, Jing-Ke Meng*, Wei-Shi Zheng
                <br>
                <em>IEEE Transactions on Multimedia (TMM)</em>, 2024.
                <br>
              </td>
            </tr>

            <tr>
              <td style="padding-left: 20px; padding-bottom: 25px ; width:100%; vertical-align:middle">
                <span class="papertitle">Continual Action Assessment via Task-Consistent Score-Discriminative Feature Distribution Modeling</span>
                <br>
                Yuan-Ming Li, <strong>Ling-An Zeng</strong>, Jing-Ke Meng*, Wei-Shi Zheng*.
                <br>
                <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em>, 2024.
                <br>
              </td>
            </tr>

            <tr>
              <td style="padding-left: 20px; padding-bottom: 25px ; width:100%; vertical-align:middle">
                <span class="papertitle">Multimodal action quality assessment</span>
                <br>
                <strong>Ling-An Zeng</strong>, Wei-Shi Zheng*.
                <br>
                <em>IEEE Transactions on Image Processing (TIP)</em>, 2024.
                <br>
              </td>
            </tr>

            <tr>
              <td style="padding-left: 20px; padding-bottom: 25px ; width:100%; vertical-align:middle">
                <span class="papertitle">Likert Scoring With Grade Decoupling for Long-Term Action Assessment</span>
                <br>
                Angchi Xu, <strong>Ling-An Zeng</strong>, Wei-Shi Zheng*
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022.
                <br>
              </td>
            </tr>

            <tr>
              <td style="padding-left: 20px; padding-bottom: 25px ; width:100%; vertical-align:middle">
                <span class="papertitle">Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos</span>
                <br>
                <strong>Ling-An Zeng</strong>, Fa-Ting Hong, Wei-Shi Zheng*, Qi-Zhi Yu, Wei Zeng, Yao-Wei Wang, Jian-Huang Lai
                <br>
                <em>ACM international conference on multimedia (ACM MM)</em>, 2020.
                <br>
              </td>
            </tr>
          </tbody></table>




          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
          <tr>
            <td style="width:100%; vertical-align:middle">
              <h2 id="service" style="scroll-margin-top: 45px;">
                Services and Activities
              </h2>
              <p>
                <div style="padding-bottom: 5px"><strong>Journal Reviewer:</strong></div>
                <div style="padding-bottom: 5px">Transactions on Machine Learning Research (TMLR)</div>
                <!-- <div style="padding-bottom: 5px">Pattern Recognition (PR)</div> -->
              </p>
              <p>
                <div style="padding-bottom: 5px"><strong>Conference Reviewer:</strong></div>
                CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML, AAAI, ACM MM, ICME.
              </p>
            </td>
          </tr>

          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
          <tr>
            <td style="width:100%; vertical-align:middle">
              <h2 id="award" style="scroll-margin-top: 45px;">
                Honours and Awards
              </h2>
              <p>
                <div style="padding-bottom: 5px">NeurIPS 2024 Outstanding Reviewer Award, 2024</div>
                <div style="padding-bottom: 5px">Meritorious Winner, Mathematical Contest in Modeling, 2018</div>
                <div style="padding-bottom: 5px">Gold Medal, ACM Sichuan Province Programming Contest, 2017</div>
                <div style="padding-bottom: 5px">Bronze Medal, ICPC-ACM Programming Contest, Shenyang Regional Contest, 2017</div>

              </p>
            </td>
          </tr>
          </tbody></table>

        </td>
      </tr>
    </table>

    <script src="script.js"></script>

    <!-- Add Schema.org structured data for scholar profile -->
    <script type="application/ld+json">
      {
        "@context": "https://schema.org/",
        "@type": "Person",
        "name": "Ling-An Zeng",
        "givenName": "Ling-An",
        "familyName": "Zeng",
        "url": "https://lingan.art",
        "image": {
          "@type": "ImageObject",
          "url": "https://lingan.art/images/myself.jpg",
          "width": "800",
          "height": "800",
          "caption": "Ling-An Zeng"
        },
        "sameAs": [
          "https://scholar.google.com/citations?hl=zh-CN&user=Uyx8l8sAAAAJ"
        ],
        "jobTitle": "PhD Student",
        "worksFor": {
          "@type": "Organization",
          "name": "Sun Yat-sen University"
        },
        "email": "linganzeng@gmail.com",
        "description": "Researcher in video understanding, human-related technologies, and AIGC",
        "alumniOf": [
          {
            "@type": "Organization",
            "name": "Sun Yat-sen University"
          },
          {
            "@type": "Organization",
            "name": "University of Electronic Science and Technology of China"
          }
        ],
        "knowsAbout": ["Video Understanding", "Motion Generation", "Human-Object Interaction", "AIGC", "Action Quality Assessment"]
      }
    </script>

  </body>
</html>
